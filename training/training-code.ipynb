{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFntrHGeW9b0"
   },
   "source": [
    "# Scegli la modalità di training\n",
    "All'avvio puoi scegliere tra:\n",
    "- **Modalità dettagliata**: include visualizzazioni, salvataggi frequenti e output completi.\n",
    "- **Modalità veloce**: elimina visualizzazioni e salva solo i checkpoint essenziali, per addestramento più rapido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p8-qlwROW9b1",
    "outputId": "966b4c07-57f9-424d-ac21-281ee9254775"
   },
   "outputs": [],
   "source": [
    "# Select training mode: 'detailed' or 'fast' (boolean flag)\n",
    "try:\n",
    "    # Colab/terminal: interactive input\n",
    "    mode_input = input(\"Select training mode: 'detailed'(d) or 'fast'(f) [detailed]: \").strip().lower()\n",
    "    if mode_input not in [\"fast\", \"detailed\", \"f\", \"d\", \"\"]:\n",
    "        print(\"Unrecognized mode, using 'detailed'.\")\n",
    "        mode_input = \"detailed\"\n",
    "    if mode_input == \"\" or mode_input in [\"detailed\", \"d\"]:\n",
    "        fast_mode = False\n",
    "    else:\n",
    "        fast_mode = True\n",
    "except Exception:\n",
    "    # Fallback if input is not available\n",
    "    fast_mode = False\n",
    "print(f\"Selected mode: {'fast' if fast_mode else 'detailed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzLcjMXkW9b2"
   },
   "source": [
    "# Animator2D con Google Drive\n",
    "Questo notebook è configurato per utilizzare Google Drive sia per caricare il dataset che per salvare i risultati dell'addestramento. Prima di iniziare, assicurati che:\n",
    "\n",
    "1. Il file `dataset.zip` sia già caricato nella root di Google Drive\n",
    "2. Lo spazio su Google Drive sia sufficiente per salvare i checkpoint e le visualizzazioni\n",
    "\n",
    "Il notebook si occuperà automaticamente di:\n",
    "- Montare Google Drive\n",
    "- Copiare e decomprimere il dataset da Drive\n",
    "- Salvare tutti i checkpoint e le visualizzazioni su Drive\n",
    "\n",
    "Ciò consentirà di conservare i risultati anche dopo la chiusura della sessione Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hKAri3KQW9b2",
    "outputId": "fe5cd04f-3161-4adc-9516-bd25e5be2447"
   },
   "outputs": [],
   "source": [
    "# Monta Google Drive per accesso ai file\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Definisci i percorsi principali su Google Drive\n",
    "DRIVE_ROOT = \"/content/drive/MyDrive/Animator2D\"\n",
    "DATASET_ZIP_PATH = f\"{DRIVE_ROOT}/dataset.zip\"\n",
    "DRIVE_CHECKPOINTS_PATH = f\"{DRIVE_ROOT}/checkpoints\"\n",
    "\n",
    "# Crea le directory su Drive se non esistono\n",
    "import os\n",
    "os.makedirs(DRIVE_ROOT, exist_ok=True)\n",
    "os.makedirs(DRIVE_CHECKPOINTS_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Google Drive montato. Percorsi configurati:\")\n",
    "print(f\"- Root: {DRIVE_ROOT}\")\n",
    "print(f\"- Dataset: {DATASET_ZIP_PATH}\")\n",
    "print(f\"- Checkpoints: {DRIVE_CHECKPOINTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suwhNZqcwNNP"
   },
   "source": [
    "# Animator2D - Caricamento Dataset e Addestramento (Pixel Art)\n",
    "Questo notebook implementa il progetto Animator2D, che genera animazioni di sprite 2D in stile pixel art a partire dal primo frame e dai metadati (azione, direzione, numero di frame). Il dataset è fornito come file zip e viene decompresso su Colab. La struttura del dataset include:\n",
    "- Cartella `image_transparent` con sottocartelle `spritesheet_{numero}` contenenti frame (`frame_0.png`, `frame_1.png`, ecc.) in formato RGBA.\n",
    "- File `sprite_metadata.json` con metadati (azione, direzione, numero di frame).\n",
    "\n",
    "## Obiettivi\n",
    "- Caricare e decomprimere lo zip del dataset.\n",
    "- Verificare la struttura del dataset.\n",
    "- Definire una classe `SpriteDataset` per gestire i dati, preservando le dimensioni originali dei frame (pixel art) e raggruppandoli per dimensioni.\n",
    "- Creare `DataLoader` separati per ogni gruppo di dimensioni.\n",
    "- Visualizzare un esempio di frame per confermare il caricamento corretto.\n",
    "- Definire il modello `Animator2D` per generare frame successivi.\n",
    "- Implementare una funzione di loss ottimizzata per pixel art.\n",
    "- Addestrare il modello usando i DataLoader.\n",
    "\n",
    "## Istruzioni\n",
    "1. Esegui ogni cella in ordine.\n",
    "2. Carica il file zip del dataset quando richiesto.\n",
    "3. Controlla gli output per verificare che i dati siano caricati correttamente.\n",
    "4. Se incontri errori, leggi i messaggi e verifica la struttura del dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZG2Nz1jwNNQ"
   },
   "source": [
    "## Cella 1: Caricamento e Decompressione dello Zip da Google Drive\n",
    "Questa cella copia e decomprime il file `dataset.zip` da Google Drive in una directory locale `/content/dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2UergSvxwNNQ",
    "outputId": "c18b7e52-c1d6-4ff8-cbe6-0e38a9ed8096"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Verifica l'esistenza del file dataset.zip su Google Drive\n",
    "if not os.path.exists(DATASET_ZIP_PATH):\n",
    "    print(f\"Errore: Il file {DATASET_ZIP_PATH} non è stato trovato su Google Drive.\")\n",
    "    print(\"Assicurati di aver caricato dataset.zip nella cartella Animator2D su Google Drive.\")\n",
    "    raise FileNotFoundError(f\"File dataset.zip non trovato in {DATASET_ZIP_PATH}\")\n",
    "else:\n",
    "    print(f\"Dataset trovato: {DATASET_ZIP_PATH}\")\n",
    "\n",
    "# Crea la directory locale per il dataset se non esiste\n",
    "dataset_dir = \"/content/dataset\"\n",
    "if os.path.exists(dataset_dir):\n",
    "    shutil.rmtree(dataset_dir)  # Rimuovi la directory se esiste già per evitare conflitti\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# Copia e decomprime il dataset da Drive\n",
    "print(f\"Decompressione del dataset da {DATASET_ZIP_PATH} a {dataset_dir}...\")\n",
    "!unzip -q \"{DATASET_ZIP_PATH}\" -d \"{dataset_dir}\"\n",
    "\n",
    "# Verifica i contenuti della cartella\n",
    "print(\"\\nContenuto della directory dataset:\")\n",
    "!ls \"{dataset_dir}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxY4Dz6rwNNR"
   },
   "source": [
    "## Cella 2: Verifica della Struttura del Dataset\n",
    "Questa cella mostra la struttura delle cartelle e dei file nel dataset per confermare che `image_transparent` e `sprite_metadata.json` siano presenti e correttamente organizzati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2mPd1z38wNNR",
    "outputId": "3d15a657-2fb7-4dd4-f0c6-28e6963dcff1"
   },
   "outputs": [],
   "source": [
    "!ls -R /content/dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJNbXtcAwNNR"
   },
   "source": [
    "## Cella 3: Installazione Librerie e Configurazione Iniziale\n",
    "Questa cella installa le librerie necessarie, importa i moduli, verifica la disponibilità della GPU e definisce il percorso del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XwNA92zD-Uu-",
    "outputId": "bb564bcb-b5bc-4652-9b71-16a271ec8456"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers imageio torch torchvision matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MlbyNpO6wNNR",
    "outputId": "857d8c67-e50d-4e32-9bbe-4a398ca17e34"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importazioni differite\n",
    "def load_torch_modules():\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from transformers import T5Tokenizer, T5EncoderModel\n",
    "    return torch, nn, T5Tokenizer, T5EncoderModel\n",
    "\n",
    "# Verifica GPU\n",
    "torch, nn, T5Tokenizer, T5EncoderModel = load_torch_modules()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo in uso: {device}\")\n",
    "\n",
    "# Percorso del dataset\n",
    "DATASET_PATH = \"/content/dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5irEJ43wNNR"
   },
   "source": [
    "## Cella 4: Definizione della Classe SpriteDataset\n",
    "Questa cella definisce la classe `SpriteDataset` per caricare i frame e i metadati. I frame sono nominati `frame_0.png`, `frame_1.png`, ecc., e si trovano nella cartella `image_transparent`. Le sequenze sono raggruppate per dimensioni per preservare lo stile pixel art."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qjopvytSwNNR"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class SpriteDataset(Dataset):\n",
    "    def __init__(self, dataset_path, transform=None):\n",
    "        self.dataset_path = Path(dataset_path)\n",
    "        self.images_path = self.dataset_path / \"train\"\n",
    "        self.metadata_path = self.dataset_path / \"sprite_metadata.json\"\n",
    "        self.transform = transform\n",
    "\n",
    "        if not self.images_path.exists():\n",
    "            print(f\"Errore: Cartella {self.images_path} non trovata.\")\n",
    "        if not self.metadata_path.exists():\n",
    "            print(f\"Errore: File {self.metadata_path} non trovato.\")\n",
    "\n",
    "        try:\n",
    "            with open(self.metadata_path, \"r\") as f:\n",
    "                self.metadata = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nel caricamento di sprite_metadata.json: {e}\")\n",
    "            self.metadata = {}\n",
    "\n",
    "        self.sequences = []\n",
    "        self.dimension_groups = {}\n",
    "        self._prepare_sequences()\n",
    "        print(f\"Totale sequenze caricate: {len(self.sequences)}\")\n",
    "        if len(self.sequences) == 0:\n",
    "            print(\"Nessuna sequenza valida trovata. Verifica la struttura del dataset.\")\n",
    "\n",
    "    def _prepare_sequences(self):\n",
    "        for key, sprite_data in self.metadata.items():\n",
    "            folder_name = sprite_data.get(\"folder_name\", key)\n",
    "            sprite_folder = self.images_path / folder_name\n",
    "            if not sprite_folder.exists():\n",
    "                print(f\"Cartella {sprite_folder} non trovata, salto sequenza {key}.\")\n",
    "                continue\n",
    "\n",
    "            available_frames = sorted(\n",
    "                [f for f in sprite_folder.glob(\"frame_*.png\")],\n",
    "                key=lambda x: int(x.stem.split(\"_\")[1])\n",
    "            )\n",
    "\n",
    "            if not available_frames:\n",
    "                print(f\"Nessun frame trovato in {sprite_folder}, salto sequenza {key}.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                expected_frames = int(sprite_data.get(\"frames\", len(available_frames)))\n",
    "            except ValueError:\n",
    "                print(f\"Valore di 'frames' non valido per sequenza {key}: {sprite_data.get('frames')}\")\n",
    "                continue\n",
    "\n",
    "            if len(available_frames) < 2:\n",
    "                print(f\"Sequenza {key} ({folder_name}) ha meno di 2 frame ({len(available_frames)}), salto.\")\n",
    "                continue\n",
    "\n",
    "            # Verifica e uniforma le dimensioni dei frame\n",
    "            try:\n",
    "                first_frame = Image.open(available_frames[0]).convert(\"RGBA\")\n",
    "                width, height = first_frame.size\n",
    "                # Verifica che tutti i frame abbiano la stessa dimensione\n",
    "                for frame_path in available_frames[1:]:\n",
    "                    frame = Image.open(frame_path).convert(\"RGBA\")\n",
    "                    if frame.size != (width, height):\n",
    "                        # Ridimensiona il frame alla dimensione del primo frame\n",
    "                        frame = frame.resize((width, height), Image.NEAREST)\n",
    "                        # Salva il frame ridimensionato (opzionale, se vuoi modificare il dataset)\n",
    "                        frame.save(frame_path)\n",
    "                        print(f\"      Ridimensionato {frame_path} a {width}x{height}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Errore nel caricamento dei frame per {folder_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if len(available_frames) != expected_frames:\n",
    "                print(f\"Avviso: Sequenza {key} ({folder_name}) ha {len(available_frames)} frame, ma ne sono attesi {expected_frames}.\")\n",
    "\n",
    "            seq_idx = len(self.sequences)\n",
    "            self.sequences.append({\n",
    "                \"folder_name\": folder_name,\n",
    "                \"frames\": available_frames,\n",
    "                \"action\": sprite_data.get(\"action\", \"unknown\"),\n",
    "                \"direction\": sprite_data.get(\"direction\", \"unknown\"),\n",
    "                \"num_frames\": len(available_frames),\n",
    "                \"dimensions\": (width, height)\n",
    "            })\n",
    "\n",
    "            dim_key = (width, height)\n",
    "            if dim_key not in self.dimension_groups:\n",
    "                self.dimension_groups[dim_key] = []\n",
    "            self.dimension_groups[dim_key].append(seq_idx)\n",
    "\n",
    "        print(f\"Gruppi di dimensioni trovati: {list(self.dimension_groups.keys())}\")\n",
    "\n",
    "    def get_dimension_groups(self):\n",
    "        return self.dimension_groups\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        frames = []\n",
    "\n",
    "        for frame_path in sequence[\"frames\"]:\n",
    "            try:\n",
    "                frame = Image.open(frame_path).convert(\"RGBA\")\n",
    "                if self.transform:\n",
    "                    frame = self.transform(frame)\n",
    "                frames.append(frame)\n",
    "            except Exception as e:\n",
    "                print(f\"Errore nel caricamento del frame {frame_path}: {e}\")\n",
    "                return None\n",
    "\n",
    "        first_frame = frames[0]\n",
    "        target_frames = frames[1:]\n",
    "\n",
    "        metadata = {\n",
    "            \"action\": sequence[\"action\"],\n",
    "            \"direction\": sequence[\"direction\"],\n",
    "            \"num_frames\": sequence[\"num_frames\"]\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"first_frame\": first_frame,\n",
    "            \"target_frames\": target_frames,\n",
    "            \"metadata\": metadata,\n",
    "            \"dimensions\": sequence[\"dimensions\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VpFWO9TwNNR"
   },
   "source": [
    "## Cella 5: Creazione dei DataLoader e Visualizzazione\n",
    "Questa cella crea `DataLoader` separati per ogni gruppo di dimensioni e visualizza un esempio di frame per confermare che il caricamento sia corretto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9tiy67CGwNNR",
    "outputId": "10908e85-aa4e-435a-9232-32c0c2fee1ca"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        return None\n",
    "\n",
    "    dimensions = batch[0][\"dimensions\"]\n",
    "    if not all(item[\"dimensions\"] == dimensions for item in batch):\n",
    "        raise ValueError(f\"Dimensioni non uniformi nel batch: {dimensions} vs {[item['dimensions'] for item in batch]}\")\n",
    "\n",
    "    first_frames = torch.stack([item[\"first_frame\"] for item in batch])\n",
    "    target_frames = [item[\"target_frames\"] for item in batch]\n",
    "    metadata = {\n",
    "        \"action\": [item[\"metadata\"][\"action\"] for item in batch],\n",
    "        \"direction\": [item[\"metadata\"][\"direction\"] for item in batch],\n",
    "        \"num_frames\": [item[\"metadata\"][\"num_frames\"] for item in batch]\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"first_frame\": first_frames,\n",
    "        \"target_frames\": target_frames,\n",
    "        \"metadata\": metadata,\n",
    "        \"dimensions\": dimensions\n",
    "    }\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = SpriteDataset(DATASET_PATH, transform=transform)\n",
    "\n",
    "data_loaders = {}\n",
    "dimension_groups = dataset.get_dimension_groups()\n",
    "\n",
    "for dim, indices in dimension_groups.items():\n",
    "    if len(indices) < 1:\n",
    "        print(f\"Gruppo di dimensioni {dim} vuoto, salto.\")\n",
    "        continue\n",
    "\n",
    "    subset = Subset(dataset, indices)\n",
    "    loader = DataLoader(\n",
    "        subset,\n",
    "        batch_size=4,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  #È un parametro per il caricamento di dati in parallelo. Inizialmente impostato a 2, ora a 0(nullo) perché più veloce\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    data_loaders[dim] = loader\n",
    "    print(f\"Creato DataLoader per dimensioni {dim} con {len(indices)} sequenze.\")\n",
    "\n",
    "for dim, loader in data_loaders.items():\n",
    "    print(f\"\\nTest DataLoader per dimensioni {dim}:\")\n",
    "    for batch in loader:\n",
    "        if batch is None:\n",
    "            print(\"Batch vuoto, salto.\")\n",
    "            continue\n",
    "        print(\"Primo frame shape:\", batch[\"first_frame\"].shape)\n",
    "        print(\"Numero di target frames per sequenza:\", [len(frames) for frames in batch[\"target_frames\"]])\n",
    "        print(\"Metadati:\", batch[\"metadata\"])\n",
    "\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        arr = batch[\"first_frame\"][0].permute(1, 2, 0).cpu().numpy()\n",
    "        plt.imshow(arr)\n",
    "        plt.title(f\"Primo frame (dim: {dim}) - trasparenza reale\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJDAfw2ZwNNS"
   },
   "source": [
    "## Cella 6: Definizione del Modello Animator2D\n",
    "Questa cella definisce il modello `Animator2D`, composto da un `TextEncoder` (per codificare i metadati testuali) e un `FrameGenerator` (per generare i frame successivi)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_Rux1NcwNNS"
   },
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name='t5-small', output_dim=512):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = T5EncoderModel.from_pretrained(model_name)\n",
    "        t5_hidden_size = self.model.config.hidden_size\n",
    "        self.projection = nn.Linear(t5_hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, actions, directions):\n",
    "        prompts = [f\"{action} {direction}\" for action, direction in zip(actions, directions)]\n",
    "        inputs = self.tokenizer(prompts, padding='longest', truncation=True, max_length=128, return_tensors='pt')\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        outputs = self.model(**inputs)\n",
    "        text_embedding = outputs.last_hidden_state[:, 0]\n",
    "        text_embedding = self.projection(text_embedding)\n",
    "        return text_embedding\n",
    "\n",
    "class FrameGenerator(nn.Module):\n",
    "    def __init__(self, text_embedding_dim=512, base_channels=64):\n",
    "        super(FrameGenerator, self).__init__()\n",
    "        self.frame_encoder = nn.Sequential(\n",
    "            nn.Conv2d(4, base_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(base_channels, base_channels * 2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(base_channels * 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_channels * 2 + text_embedding_dim, base_channels,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(base_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(base_channels, 4, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, first_frame, text_embedding):\n",
    "        frame_features = self.frame_encoder(first_frame)\n",
    "        text_embedding = text_embedding.unsqueeze(-1).unsqueeze(-1)\n",
    "        text_embedding = text_embedding.expand(-1, -1, frame_features.shape[2], frame_features.shape[3])\n",
    "        combined = torch.cat([frame_features, text_embedding], dim=1)\n",
    "        output_frame = self.decoder(combined)\n",
    "        output_frame = torch.sigmoid(output_frame)\n",
    "        return output_frame\n",
    "\n",
    "class Animator2D(nn.Module):\n",
    "    def __init__(self, text_embedding_dim=512):\n",
    "        super(Animator2D, self).__init__()\n",
    "        self.text_encoder = TextEncoder(output_dim=text_embedding_dim)\n",
    "        self.frame_generator = FrameGenerator(text_embedding_dim=text_embedding_dim)\n",
    "\n",
    "    def forward(self, first_frame, actions, directions, num_frames):\n",
    "        # Supponiamo che text_embedding sia generato da actions e directions\n",
    "        text_embedding = self.text_encoder(actions, directions)  # Implementazione ipotetica\n",
    "        generated_frames = []\n",
    "        current_frame = first_frame\n",
    "        for _ in range(num_frames):  # Genera esattamente num_frames frame\n",
    "            next_frame = self.frame_generator(current_frame, text_embedding)\n",
    "            generated_frames.append(next_frame)\n",
    "            current_frame = next_frame\n",
    "        return torch.stack(generated_frames, dim=1)  # [1, num_frames, C, H, W]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bpXxckmwNNS"
   },
   "source": [
    "## Cella 7: Definizione della Funzione di Loss\n",
    "Questa cella definisce la funzione di loss `PixelArtLoss`, che combina L1 loss e edge loss per preservare i dettagli e i bordi netti del pixel art."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mezx6xuwwNNS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PixelArtLoss(nn.Module):\n",
    "    def __init__(self, alpha_weight=0.5, color_consistency_weight=0.05, background_alpha_weight=0.3):\n",
    "        super(PixelArtLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.alpha_weight = alpha_weight\n",
    "        self.color_weight = color_consistency_weight\n",
    "        self.background_alpha_weight = background_alpha_weight\n",
    "\n",
    "    def forward(self, generated, target, first_frame=None):\n",
    "        # Perdita principale (MSE tra frame generati e target)\n",
    "        mse_loss = self.mse(generated, target)\n",
    "\n",
    "        # Penalità generale per valori di alfa bassi\n",
    "        alpha_channel = generated[:, 3, :, :]  # Canale alfa dei frame generati\n",
    "        alpha_loss = self.alpha_weight * torch.mean((1.0 - alpha_channel) ** 2)\n",
    "\n",
    "        # Penalità per consistenza e trasparenza dello sfondo\n",
    "        color_loss = 0.0\n",
    "        background_alpha_loss = 0.0\n",
    "        if first_frame is not None:\n",
    "            # Consistenza del colore (solo RGB)\n",
    "            generated_rgb = generated[:, :3, :, :]\n",
    "            first_frame_rgb = first_frame[:3, :, :].unsqueeze(0).expand_as(generated_rgb)\n",
    "            color_loss = self.color_weight * self.mse(generated_rgb, first_frame_rgb)\n",
    "\n",
    "            # Penalità per lo sfondo trasparente\n",
    "            first_frame_alpha = first_frame[3, :, :]  # Canale alfa del primo frame\n",
    "            background_mask = (first_frame_alpha == 0).float()  # Maschera dello sfondo trasparente\n",
    "            generated_alpha = generated[:, 3, :, :]  # Canale alfa dei frame generati\n",
    "            background_alpha_loss = self.background_alpha_weight * torch.mean(\n",
    "                background_mask * (generated_alpha ** 2)\n",
    "            )\n",
    "\n",
    "        total_loss = mse_loss + alpha_loss + color_loss + background_alpha_loss\n",
    "        return total_loss\n",
    "\n",
    "# Esempio di utilizzo\n",
    "criterion = PixelArtLoss(alpha_weight=0.5, color_consistency_weight=0.05, background_alpha_weight=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xqqFvPjwNNS"
   },
   "source": [
    "## Cella 8: Addestramento del Modello\n",
    "Questa cella configura e avvia l'addestramento del modello `Animator2D` usando i `DataLoader` per ogni gruppo di dimensioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKLltSWIwNNS"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from collections import Counter\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imageio\n",
    "\n",
    "def train_model(model, data_loaders, criterion, optimizer, num_epochs=20, device='cuda', save_dir='checkpoints', max_frames_to_visualize=10, resume_checkpoint=None, save_every=1):\n",
    "    model.to(device)\n",
    "\n",
    "    # Crea le directory per checkpoint e visualizzazioni\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    vis_dir = os.path.join(save_dir, 'visualizations')\n",
    "    os.makedirs(vis_dir, exist_ok=True)\n",
    "\n",
    "    # Inizializza l'epoca di partenza a 0\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Carica il checkpoint se specificato\n",
    "    if resume_checkpoint is not None:\n",
    "        try:\n",
    "            print(f\"Caricamento del checkpoint: {resume_checkpoint}\")\n",
    "            checkpoint = torch.load(resume_checkpoint, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            print(f\"Checkpoint caricato con successo. Ripresa dell'addestramento dall'epoca {start_epoch+1}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nel caricamento del checkpoint: {e}\")\n",
    "            print(\"Inizializzazione del modello da zero.\")\n",
    "\n",
    "    # Riepilogo delle lunghezze delle sequenze per ogni dimensione (stampato una sola volta all'inizio)\n",
    "    print(\"Distribuzione delle lunghezze delle sequenze per dimensione:\")\n",
    "    for dim in data_loaders.keys():\n",
    "        data_loader = data_loaders[dim]\n",
    "        num_frames_list = []\n",
    "        for batch in data_loader:\n",
    "            if batch is None:\n",
    "                continue\n",
    "            num_frames_list.extend(batch[\"metadata\"][\"num_frames\"])\n",
    "        frame_counts = Counter(num_frames_list)\n",
    "        print(f\"Dimensione {dim}:\")\n",
    "        if frame_counts:\n",
    "            # Stampa ogni lunghezza senza frecce se ci sono 2 o meno lunghezze diverse\n",
    "            if len(frame_counts) <= 2:\n",
    "                for num_frames in sorted(frame_counts.keys()):\n",
    "                    count = frame_counts[num_frames]\n",
    "                    print(f\"  - frame: {num_frames} | sequenze: {count}\")\n",
    "            else:\n",
    "                # Trova il massimo e il minimo numero di sequenze\n",
    "                counts = list(frame_counts.values())\n",
    "                max_count = max(counts)\n",
    "                min_count = min(counts)\n",
    "\n",
    "                # Identifica le lunghezze con il conteggio massimo e minimo\n",
    "                max_frames = [num_frames for num_frames, count in frame_counts.items() if count == max_count]\n",
    "                min_frames = [num_frames for num_frames, count in frame_counts.items() if count == min_count]\n",
    "\n",
    "                # In caso di parità, scegli il frame massimo/minimo in base al numero di frame\n",
    "                max_frame = max(max_frames)  # Lunghezza con più frame tra quelle con conteggio massimo\n",
    "                min_frame = min(min_frames)  # Lunghezza con meno frame tra quelle con conteggio minimo\n",
    "\n",
    "                # Stampa ogni lunghezza con la freccia appropriata\n",
    "                for num_frames in sorted(frame_counts.keys()):\n",
    "                    count = frame_counts[num_frames]\n",
    "                    arrow = \"\"\n",
    "                    if count == max_count and num_frames == max_frame:\n",
    "                        arrow = \" ↑\"  # Freccia in su per il massimo\n",
    "                    elif count == min_count and num_frames == min_frame:\n",
    "                        arrow = \" ↓\"  # Freccia in giù per il minimo\n",
    "                    print(f\"  - frame: {num_frames} | sequenze: {count}{arrow}\")\n",
    "        else:\n",
    "            print(\"  - Nessuna sequenza disponibile\")\n",
    "\n",
    "    # Calcola il numero totale di batch per tutte le dimensioni\n",
    "    total_batches_per_epoch = sum(len(data_loaders[dim]) for dim in data_loaders.keys())\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        total_loss_all_dims = 0.0\n",
    "        total_batches_processed = 0\n",
    "\n",
    "        # Barra di progresso per l'epoca\n",
    "        with tqdm(total=total_batches_per_epoch, desc=f'Epoch {epoch+1}/{num_epochs}', unit=\"batch\") as epoch_pbar:\n",
    "            for dim in data_loaders.keys():\n",
    "                data_loader = data_loaders[dim]\n",
    "                total_batches_dim = len(data_loader)\n",
    "                batches_processed_dim = 0\n",
    "\n",
    "                for batch in data_loader:\n",
    "                    if batch is None:\n",
    "                        batches_processed_dim += 1\n",
    "                        total_batches_processed += 1\n",
    "                        epoch_pbar.set_postfix({\n",
    "                            'dim': str(dim),\n",
    "                            'batch': f'{batches_processed_dim}/{total_batches_dim}',\n",
    "                            'loss': f'{total_loss_all_dims / total_batches_processed:.4f}' if total_batches_processed > 0 else '0.0000'\n",
    "                        })\n",
    "                        epoch_pbar.update(1)\n",
    "                        continue\n",
    "\n",
    "                    first_frame = batch[\"first_frame\"].to(device)\n",
    "                    target_frames = batch[\"target_frames\"]\n",
    "                    actions = batch[\"metadata\"][\"action\"]\n",
    "                    directions = batch[\"metadata\"][\"direction\"]\n",
    "                    num_frames_list = batch[\"metadata\"][\"num_frames\"]\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    total_loss = 0.0\n",
    "                    batch_size = len(first_frame)\n",
    "\n",
    "                    # Processa ogni sequenza nel batch\n",
    "                    for i in range(batch_size):\n",
    "                        num_frames = num_frames_list[i]\n",
    "                        target = torch.stack(target_frames[i]).to(device)\n",
    "                        frames_to_generate = num_frames - 1\n",
    "\n",
    "                        # Generazione dei frame\n",
    "                        generated = model(\n",
    "                            first_frame[i:i+1],\n",
    "                            [actions[i]],\n",
    "                            [directions[i]],\n",
    "                            frames_to_generate\n",
    "                        )\n",
    "\n",
    "                        # Allinea target ai frame generati\n",
    "                        if target.shape[0] < frames_to_generate:\n",
    "                            continue\n",
    "                        target = target[:frames_to_generate]\n",
    "\n",
    "                        # Verifica delle dimensioni\n",
    "                        if generated.shape[1] != target.shape[0]:\n",
    "                            min_frames = min(generated.shape[1], target.shape[0])\n",
    "                            if min_frames == 0:\n",
    "                                continue\n",
    "                            generated = generated[:, :min_frames]\n",
    "                            target = target[:min_frames]\n",
    "\n",
    "                        # Verifica delle dimensioni spaziali\n",
    "                        if generated.shape[2:] != target.shape[1:]:\n",
    "                            continue\n",
    "\n",
    "                        # Calcolo della loss\n",
    "                        loss = criterion(generated.squeeze(0), target)\n",
    "                        total_loss += loss / batch_size\n",
    "\n",
    "                    # Backpropagation\n",
    "                    if total_loss > 0:\n",
    "                        total_loss.backward()\n",
    "                        optimizer.step()\n",
    "                        total_loss_all_dims += total_loss.item()\n",
    "\n",
    "                    batches_processed_dim += 1\n",
    "                    total_batches_processed += 1\n",
    "                    epoch_pbar.set_postfix({\n",
    "                        'dim': str(dim),\n",
    "                        'batch': f'{batches_processed_dim}/{total_batches_dim}',\n",
    "                        'loss': f'{total_loss_all_dims / total_batches_processed:.4f}'\n",
    "                    })\n",
    "                    epoch_pbar.update(1)\n",
    "\n",
    "        avg_loss_epoch = total_loss_all_dims / total_batches_processed if total_batches_processed > 0 else 0\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss_epoch:.4f}\")\n",
    "\n",
    "        # Salva un checkpoint dopo un certo numero di epoche\n",
    "        current_epoch = epoch + 1\n",
    "        if current_epoch % save_every == 0 or current_epoch == num_epochs:\n",
    "            checkpoint_path = os.path.join(save_dir, f'animator2d_epoch_{current_epoch}.pth')\n",
    "            torch.save({\n",
    "                'epoch': current_epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss_epoch,\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint salvato: {checkpoint_path}\")\n",
    "\n",
    "        # Genera e salva esempi di frame per ogni dimensione\n",
    "        model.eval()\n",
    "        total_dims = len(data_loaders.keys())\n",
    "        with torch.no_grad():\n",
    "            # Barra di progresso per il salvataggio delle visualizzazioni\n",
    "            print(f\"Salvataggio di epoca {epoch+1} - In corso\", end=\"\")\n",
    "            with tqdm(total=total_dims, desc=\"\", unit=\"dim\") as vis_pbar:\n",
    "                for dim_idx, dim in enumerate(data_loaders.keys()):\n",
    "                    data_loader = data_loaders[dim]\n",
    "                    batch = random.choice(list(data_loader))\n",
    "                    if batch is None:\n",
    "                        vis_pbar.update(1)\n",
    "                        continue\n",
    "\n",
    "                    first_frame = batch[\"first_frame\"].to(device)\n",
    "                    actions = batch[\"metadata\"][\"action\"]\n",
    "                    directions = batch[\"metadata\"][\"direction\"]\n",
    "                    num_frames_list = batch[\"metadata\"][\"num_frames\"]\n",
    "\n",
    "                    # Scegli una sequenza casuale dal batch\n",
    "                    idx = random.randint(0, len(first_frame) - 1)\n",
    "                    num_frames = num_frames_list[idx]\n",
    "                    frames_to_generate = num_frames - 1\n",
    "\n",
    "                    # Genera i frame\n",
    "                    generated = model(\n",
    "                        first_frame[idx:idx+1],\n",
    "                        [actions[idx]],\n",
    "                        [directions[idx]],\n",
    "                        frames_to_generate\n",
    "                    )\n",
    "\n",
    "                    # Converti i frame generati in immagini\n",
    "                    generated = generated.squeeze(0).cpu()\n",
    "                    first_frame_img = first_frame[idx].cpu()\n",
    "\n",
    "                    # Funzione per convertire tensori in immagini RGBA\n",
    "                    def tensor_to_image_rgba(tensor):\n",
    "                        arr = tensor.detach().cpu().numpy()\n",
    "                        if arr.shape[0] == 4:\n",
    "                            arr = np.transpose(arr, (1, 2, 0))  # [H, W, 4]\n",
    "                            arr = np.clip(arr, 0, 1)\n",
    "                            arr = (arr * 255).astype(np.uint8)\n",
    "                            return arr\n",
    "                        else:\n",
    "                            arr = np.transpose(arr[:3], (1, 2, 0))\n",
    "                            arr = np.clip(arr, 0, 1)\n",
    "                            arr = (arr * 255).astype(np.uint8)\n",
    "                            return arr\n",
    "\n",
    "                    # Salva il primo frame con trasparenza reale (PNG RGBA)\n",
    "                    first_frame_path = os.path.join(vis_dir, f'epoch_{epoch+1}_dim_{dim}_first_frame.png')\n",
    "                    img_rgba = tensor_to_image_rgba(first_frame_img)\n",
    "                    Image.fromarray(img_rgba, mode=\"RGBA\").save(first_frame_path)\n",
    "\n",
    "                    # Salva i frame generati con trasparenza reale (PNG RGBA)\n",
    "                    for i in range(min(max_frames_to_visualize, generated.shape[0])):\n",
    "                        frame_path = os.path.join(vis_dir, f'epoch_{epoch+1}_dim_{dim}_generated_frame_{i+1}.png')\n",
    "                        img_rgba = tensor_to_image_rgba(generated[i])\n",
    "                        Image.fromarray(img_rgba, mode=\"RGBA\").save(frame_path)\n",
    "\n",
    "                    # Crea e salva il GIF\n",
    "                    gif_path = os.path.join(vis_dir, f'epoch_{epoch+1}_dim_{dim}_animation.gif')\n",
    "                    def create_gif(frames, output_path, fps=10):\n",
    "                        images = [tensor_to_image_rgba(frame) for frame in frames]\n",
    "                        imageio.mimsave(output_path, images, fps=fps, loop=0)\n",
    "                    create_gif(generated, gif_path, fps=10)\n",
    "\n",
    "                    vis_pbar.set_postfix({\n",
    "                        'dimensioni': f'({dim_idx+1}/{total_dims})'\n",
    "                    })\n",
    "                    vis_pbar.update(1)\n",
    "\n",
    "            print(f\"\\rSalvataggio di epoca {epoch+1} - Completato\")\n",
    "\n",
    "        model.train()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Salva il modello finale\n",
    "    final_model_path = os.path.join(save_dir, 'animator2d_final.pth')\n",
    "    torch.save({\n",
    "        'epoch': num_epochs,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': avg_loss_epoch,\n",
    "    }, final_model_path)\n",
    "    print(f\"Modello finale salvato: {final_model_path}\")\n",
    "\n",
    "    print(\"Addestramento completato.\")\n",
    "    return final_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7JFtIm5jW9b6"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def fast_train_model(model, data_loaders, criterion, optimizer, num_epochs=50, device='cuda', save_dir='checkpoints', resume_checkpoint=None, save_every=10):\n",
    "    model.to(device)\n",
    "\n",
    "    # Crea la directory per i checkpoint\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Inizializza l'epoca di partenza a 0\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Carica il checkpoint se specificato\n",
    "    if resume_checkpoint is not None:\n",
    "        try:\n",
    "            print(f\"Caricamento del checkpoint: {resume_checkpoint}\")\n",
    "            checkpoint = torch.load(resume_checkpoint, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            print(f\"Checkpoint caricato con successo. Ripresa dell'addestramento dall'epoca {start_epoch+1}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nel caricamento del checkpoint: {e}\")\n",
    "            print(\"Inizializzazione del modello da zero.\")\n",
    "\n",
    "    # Calcola il numero totale di batch per tutte le dimensioni\n",
    "    total_batches_per_epoch = sum(len(data_loaders[dim]) for dim in data_loaders.keys())\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        total_loss_all_dims = 0.0\n",
    "        total_batches_processed = 0\n",
    "\n",
    "        # Barra di progresso per l'epoca\n",
    "        with tqdm(total=total_batches_per_epoch, desc=f'Epoch {epoch+1}/{num_epochs}', unit=\"batch\") as epoch_pbar:\n",
    "            for dim in data_loaders.keys():\n",
    "                data_loader = data_loaders[dim]\n",
    "                total_batches_dim = len(data_loader)\n",
    "                batches_processed_dim = 0\n",
    "\n",
    "                for batch in data_loader:\n",
    "                    if batch is None:\n",
    "                        batches_processed_dim += 1\n",
    "                        total_batches_processed += 1\n",
    "                        epoch_pbar.update(1)\n",
    "                        continue\n",
    "\n",
    "                    first_frame = batch[\"first_frame\"].to(device)\n",
    "                    target_frames = batch[\"target_frames\"]\n",
    "                    actions = batch[\"metadata\"][\"action\"]\n",
    "                    directions = batch[\"metadata\"][\"direction\"]\n",
    "                    num_frames_list = batch[\"metadata\"][\"num_frames\"]\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    total_loss = 0.0\n",
    "                    batch_size = len(first_frame)\n",
    "\n",
    "                    # Processa ogni sequenza nel batch\n",
    "                    for i in range(batch_size):\n",
    "                        num_frames = num_frames_list[i]\n",
    "                        target = torch.stack(target_frames[i]).to(device)\n",
    "                        frames_to_generate = num_frames - 1\n",
    "\n",
    "                        # Generazione dei frame\n",
    "                        generated = model(\n",
    "                            first_frame[i:i+1],\n",
    "                            [actions[i]],\n",
    "                            [directions[i]],\n",
    "                            frames_to_generate\n",
    "                        )\n",
    "\n",
    "                        # Allinea target ai frame generati\n",
    "                        if target.shape[0] < frames_to_generate:\n",
    "                            continue\n",
    "                        target = target[:frames_to_generate]\n",
    "\n",
    "                        # Verifica delle dimensioni\n",
    "                        if generated.shape[1] != target.shape[0]:\n",
    "                            min_frames = min(generated.shape[1], target.shape[0])\n",
    "                            if min_frames == 0:\n",
    "                                continue\n",
    "                            generated = generated[:, :min_frames]\n",
    "                            target = target[:min_frames]\n",
    "\n",
    "                        # Verifica delle dimensioni spaziali\n",
    "                        if generated.shape[2:] != target.shape[1:]:\n",
    "                            continue\n",
    "\n",
    "                        # Calcolo della loss\n",
    "                        loss = criterion(generated.squeeze(0), target)\n",
    "                        total_loss += loss / batch_size\n",
    "\n",
    "                    # Backpropagation\n",
    "                    if total_loss > 0:\n",
    "                        total_loss.backward()\n",
    "                        optimizer.step()\n",
    "                        total_loss_all_dims += total_loss.item()\n",
    "\n",
    "                    batches_processed_dim += 1\n",
    "                    total_batches_processed += 1\n",
    "                    epoch_pbar.set_postfix({\n",
    "                        'loss': f'{total_loss_all_dims / total_batches_processed:.4f}' if total_batches_processed > 0 else '0.0000'\n",
    "                    })\n",
    "                    epoch_pbar.update(1)\n",
    "\n",
    "        avg_loss_epoch = total_loss_all_dims / total_batches_processed if total_batches_processed > 0 else 0\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss media: {avg_loss_epoch:.4f}\")\n",
    "\n",
    "        # Salva un checkpoint solo ogni save_every epoche\n",
    "        current_epoch = epoch + 1\n",
    "        if current_epoch % save_every == 0 or current_epoch == num_epochs:\n",
    "            checkpoint_path = os.path.join(save_dir, f'animator2d_epoch_{current_epoch}.pth')\n",
    "            torch.save({\n",
    "                'epoch': current_epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss_epoch,\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint salvato: {checkpoint_path}\")\n",
    "\n",
    "        # Libera memoria CUDA\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Salva il modello finale\n",
    "    final_model_path = os.path.join(save_dir, 'animator2d_final.pth')\n",
    "    torch.save({\n",
    "        'epoch': num_epochs,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': avg_loss_epoch,\n",
    "    }, final_model_path)\n",
    "    print(f\"Modello finale salvato: {final_model_path}\")\n",
    "\n",
    "    print(\"Addestramento veloce completato.\")\n",
    "    return final_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJKiCUFjafJg"
   },
   "source": [
    "# Avvio dell'addestramento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "id": "q-ImqEidaEiW",
    "outputId": "26d98a1b-48b1-42e1-d5aa-dd68db7103b6"
   },
   "outputs": [],
   "source": [
    "# Start training based on the selected mode\n",
    "model = Animator2D()\n",
    "criterion = PixelArtLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "if fast_mode:\n",
    "    print(\"Starting training in fast mode...\")\n",
    "    final_model_path = fast_train_model(\n",
    "        model,\n",
    "        data_loaders,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        num_epochs=50,\n",
    "        device=device,\n",
    "        save_dir=DRIVE_CHECKPOINTS_PATH + \"_fast\",\n",
    "        save_every=1\n",
    "    )\n",
    "else:\n",
    "    print(\"Starting training in detailed mode...\")\n",
    "    final_model_path = train_model(\n",
    "        model,\n",
    "        data_loaders,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        num_epochs=50,\n",
    "        device=device,\n",
    "        save_dir=DRIVE_CHECKPOINTS_PATH,\n",
    "        save_every=1\n",
    "    )\n",
    "print(f\"Final model saved to Google Drive: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dKxI5vdW9b5"
   },
   "source": [
    "# Ripresa dell'addestramento da un checkpoint\n",
    "Questa cella dimostra come riprendere l'addestramento del modello da un checkpoint salvato in precedenza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCD4OdZ2W9b5"
   },
   "outputs": [],
   "source": [
    "# Esempio di ripresa dell'addestramento da un checkpoint\n",
    "def resume_training(checkpoint_path, additional_epochs=20):\n",
    "    print(f\"Caricamento del checkpoint da {checkpoint_path}\")\n",
    "\n",
    "    # Crea una nuova istanza del modello e del suo ottimizzatore\n",
    "    model = Animator2D()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    criterion = PixelArtLoss()\n",
    "\n",
    "    # Carica le informazioni dal checkpoint per verificare l'epoca da cui riprendere\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    current_epoch = checkpoint['epoch']\n",
    "    total_epochs = current_epoch + additional_epochs\n",
    "\n",
    "    print(f\"Ripresa dell'addestramento dall'epoca {current_epoch}\")\n",
    "    print(f\"Verranno eseguite {additional_epochs} epoche aggiuntive per un totale di {total_epochs} epoche\")\n",
    "\n",
    "    # Avvia l'addestramento, specificando il checkpoint da cui riprendere\n",
    "    save_dir = 'checkpoints_continued'\n",
    "    final_model_path = train_model(\n",
    "        model, data_loaders, criterion, optimizer,\n",
    "        num_epochs=total_epochs, device=device,\n",
    "        save_dir=save_dir, resume_checkpoint=checkpoint_path,\n",
    "        save_every=5\n",
    "    )\n",
    "\n",
    "    return final_model_path\n",
    "\n",
    "# Per utilizzare questa funzione, decommentare la riga seguente e specificare il percorso al checkpoint\n",
    "# checkpoint_path = 'checkpoints/animator2d_epoch_20.pth'  # Modifica questo percorso\n",
    "# final_model_path = resume_training(checkpoint_path, additional_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMtiCKULwNNS"
   },
   "source": [
    "# Prossimi Passaggi\n",
    "Dopo aver completato l'addestramento, puoi:\n",
    "- Valutare il modello su un set di test o generare animazioni di esempio.\n",
    "- Ottimizzare iperparametri come `edge_weight` nella loss o il learning rate.\n",
    "- Migliorare l'architettura del modello, ad esempio aggiungendo più layer o sperimentando con GAN.\n",
    "\n",
    "Se incontri errori, copia il messaggio di errore e condividilo per ricevere aiuto. Assicurati che il file zip del dataset abbia la struttura corretta:\n",
    "- `image_transparent/spritesheet_0/frame_0.png`, `frame_1.png`, ecc.\n",
    "- `sprite_metadata.json` con chiavi come `0`, `1`, e campi `folder_name`, `frames`, `action`, `direction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LdBo2NU5RXKz"
   },
   "outputs": [],
   "source": [
    "# prompt: elimina una cartella fornendo il path\n",
    "\n",
    "import shutil\n",
    "\n",
    "def delete_folder(folder_path):\n",
    "  \"\"\"Deletes a folder and its contents.\n",
    "\n",
    "  Args:\n",
    "    folder_path: The path to the folder to delete.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    shutil.rmtree(folder_path)\n",
    "    print(f\"Folder '{folder_path}' deleted successfully.\")\n",
    "  except FileNotFoundError:\n",
    "    print(f\"Folder '{folder_path}' not found.\")\n",
    "  except OSError as e:\n",
    "    print(f\"Error deleting folder '{folder_path}': {e}\")\n",
    "\n",
    "# Example usage:\n",
    "folder_to_delete = \"\"  # Replace with the actual path\n",
    "delete_folder(folder_to_delete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKVBG4FYN6DV"
   },
   "outputs": [],
   "source": [
    "# prompt: scarica la cartella visualizations\n",
    "\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "def download_folder(folder_path):\n",
    "  \"\"\"Downloads a folder as a zip file.\n",
    "\n",
    "  Args:\n",
    "      folder_path: The path to the folder to download.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    shutil.make_archive('visualizations', 'zip', folder_path)\n",
    "    files.download('visualizations.zip')\n",
    "    print(f\"Folder '{folder_path}' downloaded as visualizations.zip\")\n",
    "  except FileNotFoundError:\n",
    "    print(f\"Folder '{folder_path}' not found.\")\n",
    "  except OSError as e:\n",
    "    print(f\"Error downloading folder '{folder_path}': {e}\")\n",
    "\n",
    "# Example usage: Replace 'visualizations' with the actual folder path\n",
    "download_folder('checkpoints/visualizations')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "OMtiCKULwNNS"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Animator2D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
